import os
import sqlite3
import threading
import time
import base64
import re
import json
from datetime import datetime, timedelta
from functools import wraps

import requests
from requests.adapters import HTTPAdapter
from flask import Flask, render_template, request, Response, redirect, session, url_for
from bs4 import BeautifulSoup
from apscheduler.schedulers.background import BackgroundScheduler
from waitress import serve

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
app = Flask(__name__)

# --- å®‰å…¨é…ç½® ---
# å¿…é¡»ä¿®æ”¹æ­¤å¯†é’¥ï¼Œå¦åˆ™ Session æ— æ³•ä½¿ç”¨ (ç”¨äºç™»å½•éªŒè¯)
app.secret_key = os.environ.get('SECRET_KEY', 'local_dev_secret_key_x82ns@!09zx') 
# åå°ç®¡ç†å¯†ç 
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', '123')  

# --- ä¸Šä¼ é™åˆ¶ ---
# å…è®¸æœ€å¤§è¯·æ±‚ä½“ 100MB (é˜²æ­¢ä¸Šä¼ å¤§å›¾æŠ¥é”™)
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024 

# --- é‡‡é›†æºé…ç½® ---
SITES_CONFIG = {
    "xianbao": { 
        "name": "çº¿æŠ¥åº“", 
        "domain": "https://new.xianbao.fun", 
        "list_url": "https://new.xianbao.fun/", 
        "list_selector": "tr, li", 
        "content_selector": "#mainbox article .article-content, #art-fujia" 
    },
    "iehou": { 
        "name": "çˆ±çŒ´çº¿æŠ¥", 
        "domain": "https://iehou.com", 
        "list_url": "https://iehou.com/", 
        "list_selector": "#body ul li", 
        "content_selector": ".thread-content.message, .thread-content, .message.break-all, .message" 
    }
}

# --- é“¶è¡Œå…³é”®è¯ (ç¡¬ç¼–ç ï¼Œå› æ¶‰åŠåˆ«åæ˜ å°„) ---
BANK_KEYWORDS = {
    "å†œè¡Œ": ["å†œè¡Œ", "å†œä¸šé“¶è¡Œ", "å†œ"],
    "å·¥è¡Œ": ["å·¥è¡Œ", "å·¥å•†é“¶è¡Œ", "å·¥"],
    "å»ºè¡Œ": ["å»ºè¡Œ", "å»ºè®¾é“¶è¡Œ", "å»º", "CCB"],
    "ä¸­è¡Œ": ["ä¸­è¡Œ", "ä¸­å›½é“¶è¡Œ", "ä¸­hang"]
}
ALL_BANK_VALS = [word for words in BANK_KEYWORDS.values() for word in words]

# --- è·¯å¾„é…ç½® (é€‚é… Zeabur/Docker) ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "/app/data")
if not os.path.exists(DATA_DIR): 
    os.makedirs(DATA_DIR)
DB_PATH = os.path.join(DATA_DIR, "xianbao.db")

# --- å…¨å±€å˜é‡ ---
PER_PAGE = 30
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36"}
last_scrape_time = 0
COOLDOWN_SECONDS = 30
scrape_lock = threading.Lock()

# ç½‘ç»œè¯·æ±‚ä¼˜åŒ–
session_req = requests.Session()
session_req.headers.update(HEADERS)
adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50, max_retries=2)
session_req.mount('http://', adapter)
session_req.mount('https://', adapter)

# ==========================================
# 2. è¾…åŠ©å·¥å…· & æ•°æ®åº“
# ==========================================

# ç™»å½•éªŒè¯è£…é¥°å™¨
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('is_logged_in'):
            return redirect(url_for('login', next=request.url))
        return f(*args, **kwargs)
    return decorated_function

# æ•°æ®åº“è¿æ¥ä¸åˆå§‹åŒ–
def get_db_connection():
    conn = sqlite3.connect(DB_PATH, timeout=20)
    conn.row_factory = sqlite3.Row
    conn.execute('PRAGMA journal_mode=WAL;')
    
    # æ–‡ç« è¡¨
    conn.execute('''CREATE TABLE IF NOT EXISTS articles(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT, url TEXT UNIQUE, site_source TEXT,
        match_keyword TEXT, original_time TEXT, is_top INTEGER DEFAULT 0,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
    
    # å†…å®¹ç¼“å­˜è¡¨
    conn.execute('CREATE TABLE IF NOT EXISTS article_content(url TEXT PRIMARY KEY, content TEXT, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
    
    # æ—¥å¿—è¡¨
    conn.execute('CREATE TABLE IF NOT EXISTS scrape_log(id INTEGER PRIMARY KEY AUTOINCREMENT, last_scrape TEXT)')
    conn.execute('CREATE TABLE IF NOT EXISTS visit_stats(ip TEXT PRIMARY KEY, visit_count INTEGER DEFAULT 1, last_visit TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')

    # è§„åˆ™è¡¨ (æ”¯æŒ match_scope åŒºåˆ†æ ‡é¢˜å’Œç½‘å€)
    conn.execute('''CREATE TABLE IF NOT EXISTS config_rules(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        rule_type TEXT,  -- 'white' or 'black'
        keyword TEXT,
        match_scope TEXT DEFAULT 'title', -- 'title' or 'url'
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(keyword, match_scope)
    )''')
    
    # --- æ•°æ®åº“è¿ç§»æ£€æŸ¥ (å…¼å®¹æ—§ç‰ˆæœ¬) ---
    try:
        conn.execute("ALTER TABLE config_rules ADD COLUMN match_scope TEXT DEFAULT 'title'")
    except sqlite3.OperationalError:
        pass # åˆ—å·²å­˜åœ¨ï¼Œå¿½ç•¥

    # --- åˆå§‹åŒ–é»˜è®¤è§„åˆ™ ---
    cursor = conn.cursor()
    if cursor.execute("SELECT COUNT(*) FROM config_rules").fetchone()[0] == 0:
        defaults = ["ç«‹å‡é‡‘", "çº¢åŒ…", "è¯è´¹", "å¤§æ°´", "å°æ°´", "æœ‰æ°´", "æ¯›", "æ‹›", "hang", "ä¿¡", "ç§»åŠ¨", "è”é€š",  "æ”¯ä»˜å®", "å¾®ä¿¡", "æµé‡", "è¯è´¹åˆ¸", "å……å€¼", "zfb"]
        # é»˜è®¤åªæ·»åŠ æ ‡é¢˜ç™½åå•
        cursor.executemany("INSERT OR IGNORE INTO config_rules (rule_type, keyword, match_scope) VALUES (?, ?, ?)", 
                           [('white', w, 'title') for w in defaults])
        # é»˜è®¤æ·»åŠ å‡ ä¸ªç½‘å€é»‘åå•
        cursor.executemany("INSERT OR IGNORE INTO config_rules (rule_type, keyword, match_scope) VALUES (?, ?, ?)", 
                           [('black', 'loans', 'url'), ('black', 'google_ads', 'url')])
        conn.commit()

    conn.commit()
    return conn

# è®°å½•è®¿é—® IP
def record_visit():
    try:
        ip = request.headers.get('X-Forwarded-For', request.remote_addr)
        conn = get_db_connection()
        conn.execute('''INSERT INTO visit_stats (ip, visit_count, last_visit) VALUES (?, 1, CURRENT_TIMESTAMP)
                     ON CONFLICT(ip) DO UPDATE SET visit_count = visit_count + 1, last_visit = CURRENT_TIMESTAMP''', (ip,))
        conn.commit()
        conn.close()
    except: pass

# HTML æ¸…æ´— (å¤„ç†å›¾ç‰‡é˜²ç›—é“¾)
def clean_html(html_content, site_key):
    if not html_content: return ""
    soup = BeautifulSoup(html_content, "html.parser")
    for tag in soup.find_all(True):
        if tag.name == 'img':
            src = tag.get('src', '')
            if src.startswith('/'): src = SITES_CONFIG[site_key]['domain'] + src
            # æ”¹ä¸ºä»£ç†åœ°å€
            tag.attrs = {'src': f"/img_proxy?url={src}", 'loading': 'lazy', 'style': 'max-width:100%; border-radius:8px; display:block; margin:10px 0;'}
        elif tag.name == 'a':
            tag.attrs = {'href': tag.get('href'), 'target': '_blank', 'style': 'color: #007aff; text-decoration: none;'}
    return str(soup)

# ä¸Šä¼ å›¾ç‰‡åˆ° img.scdn.io
def upload_to_img_cdn(file_binary):
    try:
        url = 'https://img.scdn.io/api/v1.php'
        files = {'image': ('upload.jpg', file_binary)}
        data = {'cdn_domain': 'img.scdn.io'}
        res = requests.post(url, files=files, data=data, timeout=30)
        
        if res.status_code == 200:
            js = res.json()
            # å…¼å®¹å¤šç§è¿”å›æ ¼å¼
            if 'url' in js: return js['url']
            if 'data' in js and isinstance(js['data'], dict) and 'url' in js['data']: return js['data']['url']
            if 'data' in js and isinstance(js['data'], str) and js['data'].startswith('http'): return js['data']
        print(f"å›¾åºŠä¸Šä¼ å¤±è´¥: {res.text}")
    except Exception as e: 
        print(f"å›¾åºŠå¼‚å¸¸: {e}")
    return None

# ==========================================
# 3. æ ¸å¿ƒé‡‡é›†é€»è¾‘
# ==========================================
def scrape_all_sites():
    if scrape_lock.locked(): return
    with scrape_lock:
        start_time = time.time()
        conn = get_db_connection()
        
        # 1. åŠ è½½è§„åˆ™
        rules = conn.execute("SELECT * FROM config_rules").fetchall()
        title_white = [r['keyword'] for r in rules if r['rule_type']=='white' and r['match_scope']=='title']
        title_black = [r['keyword'] for r in rules if r['rule_type']=='black' and r['match_scope']=='title']
        url_white   = [r['keyword'] for r in rules if r['rule_type']=='white' and r['match_scope']=='url']
        url_black   = [r['keyword'] for r in rules if r['rule_type']=='black' and r['match_scope']=='url']
        
        base_title_keywords = ALL_BANK_VALS + title_white
        
        # === âœ… æ–°å¢ï¼šæœ¬æ¬¡æ‰¹æ¬¡å»é‡é›†åˆ ===
        # è¿™ä¸ªé›†åˆåªåœ¨æœ¬æ¬¡å‡½æ•°è¿è¡ŒæœŸé—´æœ‰æ•ˆï¼Œä¸‹æ¬¡è¿è¡Œåˆä¼šæ¸…ç©º
        # ç”¨äºé˜²æ­¢ï¼šçº¿æŠ¥åº“åˆšå‘äº†ä¸€æ¡ï¼Œçˆ±çŒ´ä¹Ÿå‘äº†ä¸€æ¡ä¸€æ ·çš„ï¼Œæœ¬æ¬¡åªæ”¶å½•ä¸€æ¡
        current_batch_titles = set()
        
        site_stats = {}
        now_beijing = datetime.utcnow() + timedelta(hours=8)
        
        for site_key, config in SITES_CONFIG.items():
            try:
                session_req.headers.update({"Referer": config['domain']})
                resp = session_req.get(config['list_url'], timeout=15)
                resp.encoding = 'utf-8'
                soup = BeautifulSoup(resp.text, "html.parser")
                
                entries = []
                for item in soup.select(config['list_selector']):
                    a = item.select_one("a[href*='view'], a[href*='thread'], a[href*='post'], a[href*='.htm']") or item.find("a")
                    if not a: continue
                    
                    href = a.get("href", "")
                    full_url = href if href.startswith("http") else (config['domain'] + (href if href.startswith("/") else "/" + href))
                    title = a.get_text(strip=True)
                    
                    # === âœ… æ ¸å¿ƒé€»è¾‘ï¼šæ‰¹æ¬¡å†…å»é‡ ===
                    # 1. å¦‚æœè¿™ä¸ªæ ‡é¢˜åœ¨"æœ¬æ¬¡"æŠ“å–ä¸­å·²ç»å‡ºç°è¿‡ï¼Œè·³è¿‡
                    if title in current_batch_titles:
                        continue
                        
                    # 2. å¦‚æœæ ‡é¢˜åœ¨"å½“å‰ç«™ç‚¹"çš„åˆ—è¡¨é‡Œé‡å¤äº†(é˜²æ­¢ç½®é¡¶å¸–å’Œæ™®é€šè´´é‡å¤)ï¼Œè·³è¿‡
                    if any(e[0] == title for e in entries):
                        continue
                    # ==============================
                    
                    # --- ä¸‹é¢æ˜¯å¸¸è§„çš„é»‘ç™½åå•ç­›é€‰ ---
                    
                    # URL é»‘åå•
                    if any(bad in full_url for bad in url_black): continue
                    
                    # æ ‡é¢˜ é»‘åå•
                    if any(bad in title for bad in title_black): continue
                    
                    final_tag = None
                    
                    # URL ç™½åå•
                    if any(good in full_url for good in url_white):
                        final_tag = "ç‰¹åˆ«å…³æ³¨"
                    
                    # æ ‡é¢˜ ç™½åå•
                    if not final_tag:
                        matched_kw = next((kw for kw in base_title_keywords if kw.lower() in title.lower()), None)
                        if matched_kw:
                            final_tag = matched_kw
                            for tag_name, val_list in BANK_KEYWORDS.items():
                                if matched_kw in val_list:
                                    final_tag = tag_name
                                    break
                    
                    if not final_tag: continue
                    
                    # é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼ŒåŠ å…¥å¾…æ’å…¥åˆ—è¡¨
                    entries.append((title, full_url, site_key, final_tag, now_beijing.strftime("%H:%M")))
                    
                    # âœ… å°†æ ‡é¢˜åŠ å…¥"å·²å­˜åœ¨"é›†åˆï¼Œåç»­å¦‚æœå…¶ä»–ç«™ç‚¹ä¹Ÿæœ‰è¿™ä¸ªæ ‡é¢˜ï¼Œå°±ä¼šè¢«ä¸Šé¢æ‹¦æˆª
                    current_batch_titles.add(title)
                
                if entries:
                    conn.executemany('INSERT OR IGNORE INTO articles (title, url, site_source, match_keyword, original_time) VALUES(?,?,?,?,?)', entries)
                    site_stats[config['name']] = len(entries)
            except Exception as e:
                print(f"ç«™ç‚¹ {site_key} æŠ“å–é”™è¯¯: {e}")
        
        # æ—¥å¿—è®°å½•
        stats_str = ", ".join([f"{k}+{v}" for k,v in site_stats.items()]) if site_stats else "æ— æ–°å†…å®¹"
        log_msg = f"[{now_beijing.strftime('%Y-%m-%d %H:%M:%S')}] ä»»åŠ¡å®Œæˆ: {stats_str}"
        print(log_msg)
        
        conn.execute('INSERT INTO scrape_log(last_scrape) VALUES(?)', (log_msg,))
        conn.execute('DELETE FROM scrape_log WHERE id NOT IN (SELECT id FROM scrape_log ORDER BY id DESC LIMIT 50)')
        conn.commit()
        conn.close()

# ==========================================
# 4. Web è·¯ç”±
# ==========================================
# ================== 1. æ–°å¢ï¼šæ‰‹åŠ¨æŠ“å–æ¥å£ ==================
@app.route('/api/scrape_now')
@login_required
def api_scrape_now():
    # å¼‚æ­¥å¯åŠ¨æŠ“å–ï¼Œä¸é˜»å¡é¡µé¢
    threading.Thread(target=scrape_all_sites).start()
    # ç¨å¾®å»¶è¿Ÿä¸€ä¸‹ï¼Œè®©ç”¨æˆ·æ„Ÿè§‰â€œå·²å¯åŠ¨â€
    time.sleep(1) 
    return redirect('/admin')
# --- ç™»å½•ç›¸å…³ ---
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        if request.form.get('password') == ADMIN_PASSWORD:
            session['is_logged_in'] = True
            return redirect(request.args.get('next') or '/admin')
        else:
            return render_template('login.html', error="å¯†ç é”™è¯¯")
    return render_template('login.html')

@app.route('/logout')
def logout():
    session.clear()
    return redirect('/')

# --- é¦–é¡µ ---
@app.route('/')
def index():
    record_visit()
    tag, q, page = request.args.get('tag'), request.args.get('q'), request.args.get('page', 1, type=int)
    
    # é¦–é¡µé¦–æ¬¡åŠ è½½è§¦å‘ä¸€æ¬¡é‡‡é›†
    global last_scrape_time
    if page == 1 and not tag and not q:
        if time.time() - last_scrape_time > COOLDOWN_SECONDS:
            last_scrape_time = time.time()
            threading.Thread(target=scrape_all_sites).start()

    conn = get_db_connection()
    where = "WHERE 1=1"
    params = []
    if tag: 
        where += " AND match_keyword = ?"
        params.append(tag)
    if q: 
        where += " AND title LIKE ?"
        params.append(f"%{q}%")
    
    sql = f'SELECT * FROM articles {where} ORDER BY is_top DESC, id DESC LIMIT ? OFFSET ?'
    articles = conn.execute(sql, params + [PER_PAGE, (page-1)*PER_PAGE]).fetchall()
    total = conn.execute(f'SELECT COUNT(*) FROM articles {where}', params).fetchone()[0]
    conn.close()

    bank_list = list(BANK_KEYWORDS.keys())
    return render_template('index.html', articles=articles, current_page=page, total_pages=(total+PER_PAGE-1)//PER_PAGE, current_tag=tag, q=q, bank_list=bank_list)

# --- æ–‡ç« è¯¦æƒ… ---
@app.route("/view")
def view():
    article_id = request.args.get("id", type=int)
    conn = get_db_connection()
    row = conn.execute("SELECT * FROM articles WHERE id=?", (article_id,)).fetchone()
    
    if not row: 
        conn.close()
        return "å†…å®¹ä¸å­˜åœ¨", 404
    
    url, site_key, title = row["url"], row["site_source"], row["title"]
    
    # å°è¯•è¯»ç¼“å­˜
    cached = conn.execute("SELECT content FROM article_content WHERE url=?", (url,)).fetchone()
    content = ""
    
    if cached and cached['content']:
        # ç”¨æˆ·å‘å¸ƒçš„ç›´æ¥æ˜¾ç¤ºï¼Œé‡‡é›†çš„ç»è¿‡æ¸…æ´—
        content = cached["content"] if site_key == "user" else clean_html(cached["content"], site_key)
    elif site_key in SITES_CONFIG:
        # ç¼“å­˜æ— æ•°æ®ï¼Œå®æ—¶æŠ“å–
        try:
            r = session_req.get(url, timeout=10)
            r.encoding = 'utf-8'
            soup = BeautifulSoup(r.text, "html.parser")
            selectors = SITES_CONFIG[site_key]["content_selector"].split(',')
            node = None
            for sel in selectors:
                node = soup.select_one(sel.strip())
                if node: break
            
            if node:
                content_raw = str(node)
                conn.execute("INSERT OR REPLACE INTO article_content(url, content) VALUES(?,?)", (url, content_raw))
                conn.commit()
                content = clean_html(content_raw, site_key)
            else:
                content = f"<div class='alert alert-warning'>æ­£æ–‡æå–å¤±è´¥ï¼Œ<a href='{url}' target='_blank'>ç‚¹å‡»è®¿é—®åŸæ–‡</a></div>"
        except Exception as e:
            content = f"åŠ è½½å¤±è´¥: {e}"
    else:
        content = f"æ— æ³•åŠ è½½å†…å®¹ï¼Œ<a href='{url}' target='_blank'>ç‚¹å‡»è®¿é—®åŸæ–‡</a>"
        
    conn.close()
    return render_template("detail.html", title=title, content=content, original_url=url, time=row['original_time'])

# --- ğŸ”’ å‘å¸ƒæ–°æ–‡ç«  ---
@app.route('/publish', methods=['GET', 'POST'])
@login_required
def publish():
    if request.method == 'POST':
        title = request.form.get('title')
        raw_content = request.form.get('content')
        is_top = 1 if request.form.get('publish_mode') == 'top' else 0
        
        # åªå¤„ç† Base64 å›¾ç‰‡ä¸Šä¼ 
        def img_replacer(match):
            try:
                cdn = upload_to_img_cdn(base64.b64decode(match.group(2)))
                return f'src="{cdn}"' if cdn else match.group(0)
            except: return match.group(0)
        
        processed = re.sub(r'src="data:image\/(.*?);base64,(.*?)"', img_replacer, raw_content)
        fake_url = f"user://{int(time.time())}"
        
        conn = get_db_connection()
        conn.execute("INSERT INTO articles (title, url, site_source, match_keyword, original_time, is_top) VALUES (?,?,?,?,?,?)",
                     (title, fake_url, "user", "ç¾Šæ¯›ç²¾é€‰", "åˆšåˆš", is_top))
        conn.execute("INSERT INTO article_content (url, content) VALUES (?,?)", (fake_url, processed))
        conn.commit()
        conn.close()
        return redirect('/')
    return render_template('publish.html')

# --- ğŸ”’ åå°ç®¡ç†é¢æ¿ ---
@app.route('/admin')
@login_required
def admin_panel():
    conn = get_db_connection()
    
    # 1. è·å–è§„åˆ™
    whitelist = conn.execute("SELECT * FROM config_rules WHERE rule_type='white' ORDER BY id DESC").fetchall()
    blacklist = conn.execute("SELECT * FROM config_rules WHERE rule_type='black' ORDER BY id DESC").fetchall()
    
    # 2. è·å–æ–‡ç« åˆ—è¡¨
    my_articles = conn.execute("SELECT * FROM articles WHERE site_source='user' ORDER BY id DESC").fetchall()
    
    # 3. è·å–ç»Ÿè®¡æ•°æ® (æ–°å¢)
    total_articles = conn.execute("SELECT COUNT(*) FROM articles").fetchone()[0]
    total_visits = conn.execute("SELECT SUM(visit_count) FROM visit_stats").fetchone()[0] or 0
    
    # è·å–æœ€åæŠ“å–æ—¥å¿—
    last_log = conn.execute("SELECT last_scrape FROM scrape_log ORDER BY id DESC LIMIT 1").fetchone()
    last_scrape_time = last_log[0].split(']')[0].replace('[', '') if last_log else "æš‚æ— è®°å½•"

    conn.close()
    
    return render_template('admin.html', 
                           whitelist=whitelist, 
                           blacklist=blacklist, 
                           my_articles=my_articles,
                           stats={
                               'total_articles': total_articles,
                               'total_visits': total_visits,
                               'last_update': last_scrape_time
                           })

# --- ğŸ”’ è§„åˆ™ç®¡ç† API ---
@app.route('/api/rule', methods=['POST'])
@login_required
def api_rule():
    action = request.form.get('action')
    rtype = request.form.get('type')  # white/black
    scope = request.form.get('scope') # title/url
    kw = request.form.get('keyword', '').strip()
    rid = request.form.get('id')
    
    conn = get_db_connection()
    if action == 'add' and kw:
        try: 
            conn.execute("INSERT INTO config_rules (rule_type, keyword, match_scope) VALUES (?, ?, ?)", (rtype, kw, scope))
        except: pass
    elif action == 'delete' and rid:
        conn.execute("DELETE FROM config_rules WHERE id=?", (rid,))
    conn.commit()
    conn.close()
    return redirect('/admin')

# --- ğŸ”’ ç¼–è¾‘æ–‡ç«  ---
@app.route('/article/edit/<int:aid>', methods=['GET', 'POST'])
@login_required
def edit_article(aid):
    conn = get_db_connection()
    
    if request.method == 'POST':
        title = request.form.get('title')
        raw_content = request.form.get('content')
        is_top = 1 if request.form.get('publish_mode') == 'top' else 0
        
        # åªä¸Šä¼ æ–°ç²˜è´´çš„ Base64 å›¾ç‰‡
        def img_replacer(match):
            try:
                cdn = upload_to_img_cdn(base64.b64decode(match.group(2)))
                return f'src="{cdn}"' if cdn else match.group(0)
            except: return match.group(0)
            
        processed = re.sub(r'src="data:image\/(.*?);base64,(.*?)"', img_replacer, raw_content)
        
        row = conn.execute("SELECT url FROM articles WHERE id=?", (aid,)).fetchone()
        if row:
            conn.execute("UPDATE articles SET title=?, is_top=? WHERE id=?", (title, is_top, aid))
            conn.execute("UPDATE article_content SET content=? WHERE url=?", (processed, row['url']))
            conn.commit()
        
        conn.close()
        return redirect('/admin')

    article = conn.execute("SELECT * FROM articles WHERE id=? AND site_source='user'", (aid,)).fetchone()
    if not article: return "æœªæ‰¾åˆ°æ–‡ç« ", 404
    
    content = conn.execute("SELECT content FROM article_content WHERE url=?", (article['url'],)).fetchone()['content']
    conn.close()
    return render_template('edit.html', article=article, content=content)

# --- ğŸ”’ åˆ é™¤æ–‡ç«  ---
@app.route('/article/delete/<int:aid>')
@login_required
def delete_article(aid):
    conn = get_db_connection()
    row = conn.execute("SELECT url FROM articles WHERE id=? AND site_source='user'", (aid,)).fetchone()
    if row:
        conn.execute("DELETE FROM articles WHERE id=?", (aid,))
        conn.execute("DELETE FROM article_content WHERE url=?", (row['url'],))
        conn.commit()
    conn.close()
    return redirect('/admin')

# --- ğŸ”’ ç³»ç»Ÿæ—¥å¿— ---
@app.route('/logs')
@login_required
def show_logs():
    conn = get_db_connection()
    logs = conn.execute('SELECT last_scrape FROM scrape_log ORDER BY id DESC LIMIT 50').fetchall()
    visitors = conn.execute('SELECT * FROM visit_stats ORDER BY last_visit DESC LIMIT 30').fetchall()
    conn.close()
    return render_template('logs.html', logs=logs, visitors=visitors)

# --- å›¾ç‰‡ä»£ç† (é˜²ç›—é“¾) ---
@app.route('/img_proxy')
def img_proxy():
    url = request.args.get('url')
    if not url: return "", 404
    try:
        r = requests.get(url, headers=HEADERS, stream=True, timeout=10)
        return Response(r.content, content_type=r.headers.get('Content-Type'))
    except: return Response(status=404)

# ==========================================
# 5. å¯åŠ¨å…¥å£
# ==========================================
if __name__ == '__main__':
    # åˆå§‹åŒ– DB
    get_db_connection().close()
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    scheduler = BackgroundScheduler()
    scheduler.add_job(scrape_all_sites, 'interval', minutes=10)
    scheduler.start()
    
    # å¯åŠ¨æ—¶ç«‹å³æŠ“ä¸€æ¬¡
    threading.Thread(target=scrape_all_sites).start()
    
    print("Waitress æœåŠ¡å™¨å¯åŠ¨ä¸­: http://0.0.0.0:8080")
    # max_request_body_size è®¾ç½®ä¸º 100MBï¼Œè§£å†³ Request Entity Too Large

    serve(app, host='0.0.0.0', port=8080, threads=10, max_request_body_size=104857600)
