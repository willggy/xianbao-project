import os
import sqlite3
import threading
import time
import base64
import re
from datetime import datetime, timedelta
from functools import wraps

import requests
from requests.adapters import HTTPAdapter
from flask import Flask, render_template, request, Response, redirect, session, url_for
from bs4 import BeautifulSoup
from apscheduler.schedulers.background import BackgroundScheduler
from waitress import serve

# ==========================================
# 1. åŸºç¡€é…ç½®
# ==========================================
app = Flask(__name__)

SITE_TITLE = "å¤å¸Œè…ŠæŒç®¡ç¾Šæ¯›çš„ç¥"
app.secret_key = os.environ.get('SECRET_KEY', 'xianbao_secret_key_888') 
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', '123')  
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024 

# é‡‡é›†æºé…ç½®
SITES_CONFIG = {
    "xianbao": { 
        "name": "çº¿æŠ¥åº“", 
        "domain": "https://new.xianbao.fun", 
        "list_url": "https://new.xianbao.fun/", 
        "list_selector": "tr, li",
        "content_selector": "#mainbox article .article-content, #art-fujia"
    },
    "iehou": { 
        "name": "çˆ±çŒ´çº¿æŠ¥", 
        "domain": "https://iehou.com", 
        "list_url": "https://iehou.com/", 
        "list_selector": "#body ul li",
        "content_selector": ".thread-content.message, .thread-content, .message.break-all, .message"
    }
}

# é“¶è¡Œè‡ªåŠ¨åˆ†ç±»é€»è¾‘ (ä¿ç•™åˆ«åæ˜ å°„)
BANK_KEYWORDS = {
    "å†œè¡Œ": ["å†œè¡Œ", "å†œä¸šé“¶è¡Œ", "å†œ", "nh"],
    "å·¥è¡Œ": ["å·¥è¡Œ", "å·¥å•†é“¶è¡Œ", "å·¥", "gh"],
    "å»ºè¡Œ": ["å»ºè¡Œ", "å»ºè®¾é“¶è¡Œ", "å»º", "CCB", "jh"],
    "ä¸­è¡Œ": ["ä¸­è¡Œ", "ä¸­å›½é“¶è¡Œ", "ä¸­hang"]
}
ALL_BANK_VALS = [word for words in BANK_KEYWORDS.values() for word in words]

# æ•°æ®åº“è·¯å¾„ (é€‚é… Zeabur)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)
DB_PATH = os.path.join(DATA_DIR, "xianbao.db")

PER_PAGE = 30
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"}

session_http = requests.Session()
session_http.headers.update(HEADERS)
adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50, max_retries=2)
session_http.mount('http://', adapter)
session_http.mount('https://', adapter)

scrape_lock = threading.Lock()

# ==========================================
# 2. æ•°æ®åº“ä¸æƒé™
# ==========================================
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('is_logged_in'):
            return redirect(url_for('login', next=request.url))
        return f(*args, **kwargs)
    return decorated_function

def get_db_connection():
    conn = sqlite3.connect(DB_PATH, timeout=60)
    conn.row_factory = sqlite3.Row
    conn.execute('PRAGMA journal_mode=WAL;')
    
    # æ–‡ç« è¡¨
    conn.execute('''CREATE TABLE IF NOT EXISTS articles(
        id INTEGER PRIMARY KEY AUTOINCREMENT, 
        title TEXT, url TEXT UNIQUE, site_source TEXT,
        match_keyword TEXT, original_time TEXT, is_top INTEGER DEFAULT 0,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
    
    # åŠ¨æ€è§„åˆ™è¡¨
    conn.execute('''CREATE TABLE IF NOT EXISTS config_rules(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        rule_type TEXT,  -- 'white' or 'black'
        keyword TEXT,
        match_scope TEXT DEFAULT 'title', -- 'title' or 'url'
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(keyword, match_scope))''')
    
    conn.execute('CREATE TABLE IF NOT EXISTS article_content(url TEXT PRIMARY KEY, content TEXT, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
    conn.execute('CREATE TABLE IF NOT EXISTS scrape_log(id INTEGER PRIMARY KEY AUTOINCREMENT, last_scrape TEXT)')
    conn.execute('CREATE TABLE IF NOT EXISTS visit_stats(ip TEXT PRIMARY KEY, visit_count INTEGER DEFAULT 1, last_visit TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
    
    # åˆå§‹åŒ–è§„åˆ™
    cursor = conn.cursor()
    if cursor.execute("SELECT COUNT(*) FROM config_rules").fetchone()[0] == 0:
        defaults = ["ç«‹å‡é‡‘", "çº¢åŒ…", "è¯è´¹", "æ°´", "æ¯›", "æ‹›", "ä¿¡", "ç§»åŠ¨", "è”é€š", "äº¬ä¸œ", "æ”¯ä»˜å®", "å¾®ä¿¡", "æµé‡", "å……å€¼", "zfb"]
        cursor.executemany("INSERT OR IGNORE INTO config_rules (rule_type, keyword, match_scope) VALUES (?, ?, ?)", 
                           [('white', w, 'title') for w in defaults])
        conn.commit()

    conn.commit()
    return conn

def record_visit():
    ua = request.headers.get('User-Agent', '')
    if 'HealthCheck' in ua or 'Zeabur' in ua: return
    ip = request.headers.get('X-Forwarded-For', request.remote_addr)
    try:
        conn = get_db_connection()
        conn.execute('''INSERT INTO visit_stats (ip, visit_count, last_visit) VALUES (?, 1, CURRENT_TIMESTAMP)
                     ON CONFLICT(ip) DO UPDATE SET visit_count = visit_count + 1, last_visit = CURRENT_TIMESTAMP''', (ip,))
        conn.commit()
        conn.close()
    except: pass

# ==========================================
# 3. æ ¸å¿ƒæŠ“å–é€»è¾‘ (è§„åˆ™åŒ¹é…)
# ==========================================
def scrape_all_sites():
    if scrape_lock.locked(): return
    with scrape_lock:
        now_beijing = datetime.utcnow() + timedelta(hours=8)
        conn = get_db_connection()
        
        # åŠ è½½åŠ¨æ€è§„åˆ™
        rules = conn.execute("SELECT * FROM config_rules").fetchall()
        title_white = [r['keyword'] for r in rules if r['rule_type']=='white' and r['match_scope']=='title']
        title_black = [r['keyword'] for r in rules if r['rule_type']=='black' and r['match_scope']=='title']
        url_white   = [r['keyword'] for r in rules if r['rule_type']=='white' and r['match_scope']=='url']
        url_black   = [r['keyword'] for r in rules if r['rule_type']=='black' and r['match_scope']=='url']
        
        base_keywords = ALL_BANK_VALS + title_white
        site_stats = {}

        for site_key, config in SITES_CONFIG.items():
            try:
                resp = session_http.get(config['list_url'], timeout=15)
                resp.encoding = 'utf-8'
                soup = BeautifulSoup(resp.text, "html.parser")
                
                new_count = 0
                for item in soup.select(config['list_selector']):
                    a = item.select_one("a[href*='view'], a[href*='thread'], a[href*='post']") or item.find("a")
                    if not a: continue
                    
                    href = a.get("href", "")
                    full_url = href if href.startswith("http") else (config['domain'] + (href if href.startswith("/") else "/" + href))
                    title = a.get_text(strip=True)
                    
                    # åŠ¨æ€é»‘åå•è¿‡æ»¤
                    if any(bad in full_url for bad in url_black): continue
                    if any(bad in title for bad in title_black): continue
                    
                    final_tag = None
                    # ç½‘å€ç™½åå•åŒ¹é…
                    if any(good in full_url for good in url_white):
                        final_tag = "ç‰¹åˆ«å…³æ³¨"
                    
                    # æ ‡é¢˜å…³é”®è¯åŒ¹é…
                    if not final_tag:
                        matched_kw = next((kw for kw in base_keywords if kw.lower() in title.lower()), None)
                        if matched_kw:
                            final_tag = matched_kw
                            for tag_name, val_list in BANK_KEYWORDS.items():
                                if matched_kw in val_list: final_tag = tag_name; break
                    
                    if not final_tag: continue
                    
                    try:
                        conn.execute('INSERT OR IGNORE INTO articles (title, url, site_source, match_keyword, original_time) VALUES(?,?,?,?,?)',
                                     (title, full_url, site_key, final_tag, now_beijing.strftime("%H:%M")))
                        if conn.total_changes > 0: new_count += 1
                    except: pass
                site_stats[config['name']] = new_count
            except: pass

        # æ¸…ç† 4 å¤©å‰æ—§æ•°æ®
        conn.execute("DELETE FROM articles WHERE site_source != 'user' AND updated_at < datetime('now', '-4 days')")
        conn.execute("DELETE FROM article_content WHERE url NOT IN (SELECT url FROM articles)")
        conn.commit()
        
        log_msg = f"[{now_beijing.strftime('%Y-%m-%d %H:%M:%S')}] ä»»åŠ¡å®Œæˆ: {site_stats}"
        conn.execute('INSERT INTO scrape_log(last_scrape) VALUES(?)', (log_msg,))
        conn.commit(); conn.close()

# ==========================================
# 4. è·¯ç”±ä¸ API
# ==========================================
@app.route('/')
def index():
    record_visit()
    
    # --- ğŸ•’ æ–°å¢ï¼šè®¡ç®—ä¸‹æ¬¡åˆ·æ–°æ—¶é—´é€»è¾‘ ---
    # è·å–å½“å‰åŒ—äº¬æ—¶é—´
    now = datetime.utcnow() + timedelta(hours=8)
    # è®¡ç®—è·ç¦»ä¸‹ä¸€ä¸ª 5 åˆ†é’Ÿæ•´ç‚¹è¿˜æœ‰å‡ åˆ†é’Ÿ
    remain_mins = 5 - (now.minute % 5)
    next_refresh_dt = now + timedelta(minutes=remain_mins)
    # æ ¼å¼åŒ–ä¸º 20:05 è¿™ç§å½¢å¼
    next_refresh_time = next_refresh_dt.strftime("%H:%M")
    # -------------------------------

    tag, q, page = request.args.get('tag'), request.args.get('q'), request.args.get('page', 1, type=int)
    conn = get_db_connection()
    where = "WHERE 1=1"
    params = []
    if tag: where += " AND match_keyword = ?"; params.append(tag)
    if q: where += " AND title LIKE ?"; params.append(f"%{q}%")
    
    articles = conn.execute(f'SELECT * FROM articles {where} ORDER BY is_top DESC, id DESC LIMIT ? OFFSET ?', 
                            params + [PER_PAGE, (page-1)*PER_PAGE]).fetchall()
    total = conn.execute(f'SELECT COUNT(*) FROM articles {where}', params).fetchone()[0]
    conn.close()

    return render_template('index.html', 
                           articles=articles, 
                           site_title=SITE_TITLE, 
                           bank_list=list(BANK_KEYWORDS.keys()), 
                           current_tag=tag, 
                           q=q, 
                           current_page=page, 
                           total_pages=(total+PER_PAGE-1)//PER_PAGE,
                           next_refresh_time=next_refresh_time) # å…³é”®ï¼šä¼ ç»™å‰ç«¯

@app.route('/admin')
@login_required
def admin_panel():
    conn = get_db_connection()
    whitelist = conn.execute("SELECT * FROM config_rules WHERE rule_type='white' ORDER BY match_scope DESC").fetchall()
    blacklist = conn.execute("SELECT * FROM config_rules WHERE rule_type='black' ORDER BY match_scope DESC").fetchall()
    my_articles = conn.execute("SELECT * FROM articles WHERE site_source='user' ORDER BY id DESC").fetchall()
    
    # ç»Ÿè®¡æ•°æ®
    total_articles = conn.execute("SELECT COUNT(*) FROM articles").fetchone()[0]
    total_visits = conn.execute("SELECT SUM(visit_count) FROM visit_stats").fetchone()[0] or 0
    last_log = conn.execute("SELECT last_scrape FROM scrape_log ORDER BY id DESC LIMIT 1").fetchone()
    last_update = last_log[0] if last_log else "æš‚æ— è®°å½•"

    conn.close()
    return render_template('admin.html', whitelist=whitelist, blacklist=blacklist, my_articles=my_articles, 
                           stats={'total_articles': total_articles, 'total_visits': total_visits, 'last_update': last_update})

@app.route('/api/rule', methods=['POST'])
@login_required
def api_rule():
    action = request.form.get('action')
    rtype = request.form.get('type')  # white/black
    scope = request.form.get('scope') # title/url
    kw = request.form.get('keyword', '').strip()
    rid = request.form.get('id')
    
    conn = get_db_connection()
    if action == 'add' and kw:
        try: conn.execute("INSERT INTO config_rules (rule_type, keyword, match_scope) VALUES (?, ?, ?)", (rtype, kw, scope))
        except: pass
    elif action == 'delete' and rid:
        conn.execute("DELETE FROM config_rules WHERE id=?", (rid,))
    conn.commit(); conn.close()
    return redirect('/admin')

@app.route('/api/scrape_now')
@login_required
def api_scrape_now():
    threading.Thread(target=scrape_all_sites).start()
    return redirect('/admin')

@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        if request.form.get('password') == ADMIN_PASSWORD:
            session['is_logged_in'] = True
            return redirect('/admin')
    return render_template('login.html')

@app.route('/logout')
def logout():
    session.clear(); return redirect('/')

# (æ­¤å¤„å»ºè®®ä¿ç•™ app (1).py æˆ– app (2).py ä¸­çš„ /view, /publish, /img_proxy ç­‰åŠŸèƒ½è·¯ç”±ä»£ç )

# ==========================================
# 5. å¯åŠ¨
# ==========================================
if __name__ == '__main__':
    get_db_connection().close()
    scheduler = BackgroundScheduler()
    scheduler.add_job(scrape_all_sites, 'interval', minutes=5)
    scheduler.start()
    threading.Thread(target=scrape_all_sites).start()
    serve(app, host='0.0.0.0', port=8080, threads=10, max_request_body_size=104857600)
