# crawler.py - é€‚ç”¨äº Zeabur MySQL ç›´å†™
import requests
from bs4 import BeautifulSoup
import os
import mysql.connector
from datetime import datetime
import time

# --- æ•°æ®åº“é…ç½®åŒº ---
# çˆ¬è™«ä½œä¸ºç‹¬ç«‹æœåŠ¡è¿è¡Œï¼Œç›´æ¥ä½¿ç”¨ Zeabur æ³¨å…¥çš„ MySQL ç¯å¢ƒå˜é‡
MYSQL_CONFIG = {
    'host': os.environ.get('DB_HOST', 'localhost'),
    'user': os.environ.get('DB_USER', 'root'),
    'password': os.environ.get('DB_PASSWORD', 'your_password'),
    'database': os.environ.get('DB_DATABASE', 'xianbao_db'),
    'port': os.environ.get('DB_PORT', 3306),
}

# çˆ¬è™«å’Œæ¸…ç†é…ç½®
MAX_RECORDS = 200 # æœ€å¤šä¿å­˜200æ¡æ•°æ®

# --- çˆ¬è™«é…ç½® ---
TARGET_DOMAIN = "https://new.xianbao.fun"
KEYWORDS = ["hang", "è¡Œ", "ç«‹å‡é‡‘", "ljj", "æ°´", "çº¢åŒ…", "åˆ¸"] 
EXCLUSION_KEYWORDS = ["æ’è¡Œæ¦œ", "æ’ è¡Œ æ¦œ", "æ¦œå•"] 
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': TARGET_DOMAIN
}

# --- æ•°æ®åº“æ“ä½œå‡½æ•° ---

def get_mysql_conn():
    """è¿æ¥åˆ° Zeabur æä¾›çš„ MySQL æ•°æ®åº“"""
    try:
        conn = mysql.connector.connect(**MYSQL_CONFIG)
        return conn
    except mysql.connector.Error as err:
        print(f"è‡´å‘½é”™è¯¯: MySQLè¿æ¥å¤±è´¥: {err}")
        raise

def init_db(conn):
    """ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨ (ä¸ app.py ä¸­ä¿æŒä¸€è‡´)"""
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INT AUTO_INCREMENT PRIMARY KEY,
            title VARCHAR(512) NOT NULL,
            url VARCHAR(2048) UNIQUE NOT NULL,
            match_keyword VARCHAR(100),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS article_content (
            id INT AUTO_INCREMENT PRIMARY KEY,
            url VARCHAR(2048) UNIQUE NOT NULL,
            content MEDIUMTEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            FOREIGN KEY (url) REFERENCES articles(url)
        )
    """)
    conn.commit()
    c.close()

def save_article(conn, title, url, match_kw):
    """ä¿å­˜æˆ–æ›´æ–°æ–‡ç« æ•°æ® (ä½¿ç”¨ ON DUPLICATE KEY UPDATE)"""
    c = conn.cursor()
    # MySQL è¯­æ³•ï¼šINSERT ... ON DUPLICATE KEY UPDATE
    try:
        sql = '''
            INSERT INTO articles (title, url, match_keyword)
            VALUES (%s, %s, %s)
            ON DUPLICATE KEY UPDATE 
                title = VALUES(title), 
                match_keyword = VALUES(match_keyword),
                updated_at = CURRENT_TIMESTAMP()
        '''
        c.execute(sql, (title, url, match_kw))
        conn.commit()
        return c.rowcount > 0 # è¿”å›æ˜¯å¦æˆåŠŸæ’å…¥æˆ–æ›´æ–°
    except Exception as e:
        print(f"æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
        return False
    finally:
        c.close()

def cleanup_old_records(conn):
    """æ¸…ç†æ—§è®°å½•ï¼Œä¿æŒæœ€å¤š MAX_RECORDS æ¡"""
    c = conn.cursor()
    c.execute('SELECT COUNT(*) FROM articles')
    count = c.fetchone()[0]
    
    if count > MAX_RECORDS:
        delete_count = count - MAX_RECORDS
        # MySQL é€»è¾‘ï¼šå…ˆæ‰¾å‡ºæœ€æ—©çš„è®°å½• IDï¼Œç„¶ååˆ é™¤
        c.execute(f'''
            DELETE FROM articles 
            ORDER BY created_at ASC 
            LIMIT {delete_count}
        ''')
        
        # ç®€å•æ¸…ç† content è¡¨ä¸­æ²¡æœ‰å¯¹åº” articles çš„è®°å½• (éœ€è¦å¤–é”®æ”¯æŒ)
        # ä¹Ÿå¯ä»¥æ‰‹åŠ¨æ¸…ç†ï¼Œæ­¤å¤„ä¸ºäº†ç®€åŒ–ï¼Œä»…æ¸…ç† articles
        
        conn.commit()
        print(f"MySQLå·²æ¸…ç† {delete_count} æ¡æ—§è®°å½•")
    
    c.close()

# --- çˆ¬è™«æ ¸å¿ƒé€»è¾‘ ---

def run_crawler():
    """è¿è¡Œçˆ¬è™«ä¸»å‡½æ•°"""
    print(f"[{datetime.now()}] ğŸš€ çˆ¬è™«å¯åŠ¨ï¼Œç›®æ ‡: {TARGET_DOMAIN}")
    
    conn = None
    try:
        conn = get_mysql_conn()
        init_db(conn) # ç¡®ä¿è¡¨å­˜åœ¨
        
        resp = requests.get(TARGET_DOMAIN + "/", headers=HEADERS, timeout=30)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        rows = soup.find_all('tr') 
        if not rows: 
            rows = soup.find_all('li')
        
        total_saved = 0
        
        for row in rows:
            link = row.find('a')
            if not link: continue
            
            title = link.get_text().strip()
            title_lower = title.lower()
            
            # æ’é™¤å…³é”®è¯è¿‡æ»¤
            if any(ex_kw.lower() in title_lower for ex_kw in EXCLUSION_KEYWORDS):
                continue
            
            href = link.get('href')
            if href and not href.startswith('http'):
                href = TARGET_DOMAIN + (href if href.startswith('/') else '/' + href)
            
            # å…³é”®è¯åŒ¹é…å’Œä¿å­˜
            for kw in KEYWORDS:
                if kw.lower() in title_lower:
                    if save_article(conn, title, href, kw):
                        total_saved += 1
                    break
        
        print(f"[{datetime.now()}] âœ… çˆ¬è™«å®Œæˆã€‚æ€»å…±å¤„ç†äº† {len(rows)} æ¡æ•°æ®ï¼Œä¿å­˜/æ›´æ–°äº† {total_saved} æ¡è®°å½•ã€‚")
        cleanup_old_records(conn) # æ¸…ç†æ—§è®°å½•
        
    except requests.exceptions.RequestException as e:
        print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except mysql.connector.Error as e:
        print(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
    except Exception as e:
        print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        if conn:
            conn.close()

if __name__ == '__main__':
    run_crawler()